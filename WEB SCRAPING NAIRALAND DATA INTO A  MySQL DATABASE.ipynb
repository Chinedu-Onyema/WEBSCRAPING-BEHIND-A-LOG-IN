{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264ae148",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I could just scrap the website directly if it not behind a log in\n",
    "# But i couldn't because it is behind a log in\n",
    "\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "url = 'https://www.nairaland.com/jobs'\n",
    "page = requests.get(url)\n",
    "soup = BeautifulSoup(page.text, 'html.parser')\n",
    "print(soup)\n",
    "\n",
    "\n",
    "# when my data is behind a log in, i could use the cookies\n",
    "# and headers from the website by copying the cURL as bash\n",
    "# from the network category in the developer tools and pasting\n",
    "# it in https://curlconverter.com/ which organiises the cookies and headers of the website\n",
    "# in a dictionary and then i can import the beautiful soup and request libraries.\n",
    "\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "cookies = {\n",
    "    'ai_user': 'e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z',\n",
    "    'session': '\"\"',\n",
    "    '_gid': 'GA1.2.927634570.1727849250',\n",
    "    '__gads': 'ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA',\n",
    "    '__gpi': 'UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w',\n",
    "    '__eoi': 'ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44',\n",
    "    'FCNEC': '%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D',\n",
    "    '_ga_GGM2XGB2C1': 'GS1.1.1727849249.3.1.1727849305.0.0.0',\n",
    "    '_ga': 'GA1.1.117970615.1727064250',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    # 'cookie': 'ai_user=e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z; session=\"\"; _gid=GA1.2.927634570.1727849250; __gads=ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA; __gpi=UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w; __eoi=ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44; FCNEC=%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D; _ga_GGM2XGB2C1=GS1.1.1727849249.3.1.1727849305.0.0.0; _ga=GA1.1.117970615.1727064250',\n",
    "    'priority': 'u=0, i',\n",
    "    'referer': 'https://www.nairaland.com/',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "response = requests.get('https://www.nairaland.com/jobs', cookies=cookies, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(soup)\n",
    "\n",
    "\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the main poster's name\n",
    "poster_name_tag = soup.find('a', href='/laurelp')\n",
    "poster_name = poster_name_tag.text if poster_name_tag else 'N/A'\n",
    "\n",
    "# Extract the number of posts and views\n",
    "post_views_info = soup.find('span', class_='s')\n",
    "posts_and_views = post_views_info.find_all('b')\n",
    "post_count = posts_and_views[1].text if len(posts_and_views) > 1 else 'N/A'\n",
    "views = posts_and_views[2].text if len(posts_and_views) > 2 else 'N/A'\n",
    "\n",
    "# Extract the time and date of the post\n",
    "time = post_views_info.find_all('b')[3].text if len(posts_and_views) > 3 else 'N/A'\n",
    "date = post_views_info.find_all('b')[4].text if len(posts_and_views) > 4 else 'N/A'\n",
    "\n",
    "# Extract the second poster's name (last poster)\n",
    "last_poster_tag_name = soup.find('a', href= '/seundy')  # Get the last poster from the <a> tag list\n",
    "last_poster = last_poster_tag_name.text if last_poster_tag_name else 'N/A'\n",
    "\n",
    "# Print the extracted data\n",
    "print(f\"Poster Name: {poster_name}\")\n",
    "print(f\"Post Count: {post_count}\")\n",
    "print(f\"Views: {views}\")\n",
    "print(f\"Time: {time}\")\n",
    "print(f\"Date: {date}\")\n",
    "print(f\"Last Poster Name: {last_poster}\")\n",
    "\n",
    "\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all post blocks (adjust the tag and class according to actual structure)\n",
    "posts = soup.find_all('span', class_='s')  # Assuming each post details are within <span> with class 's'\n",
    "\n",
    "# Initialize lists to store the extracted information\n",
    "poster_names = []\n",
    "post_counts = []\n",
    "view_counts = []\n",
    "times = []\n",
    "dates = []\n",
    "last_posters = []\n",
    "\n",
    "# Loop through each post block to extract information\n",
    "for post_views_info in posts:\n",
    "    # Extract poster's name\n",
    "    poster_name_tag = post_views_info.find('a')  # Assuming the first <a> tag is the poster's name\n",
    "    poster_name = poster_name_tag.text if poster_name_tag else 'N/A'\n",
    "    poster_names.append(poster_name)\n",
    "\n",
    "    # Extract the number of posts and views\n",
    "    posts_and_views = post_views_info.find_all('b')\n",
    "    post_count = posts_and_views[1].text if len(posts_and_views) > 1 else 'N/A'\n",
    "    post_counts.append(post_count)\n",
    "\n",
    "    view_count = posts_and_views[2].text if len(posts_and_views) > 2 else 'N/A'\n",
    "    view_counts.append(view_count)\n",
    "\n",
    "    # Extract the time and date of the post\n",
    "    time = posts_and_views[3].text if len(posts_and_views) > 3 else 'N/A'\n",
    "    times.append(time)\n",
    "\n",
    "    date = posts_and_views[4].text if len(posts_and_views) > 4 else 'N/A'\n",
    "    dates.append(date)\n",
    "\n",
    "    # Extract the second poster's name (last poster)\n",
    "    last_poster_tag = post_views_info.find_all('a')[-1]  # Assuming the last <a> tag is the last poster's name\n",
    "    last_poster = last_poster_tag.text if last_poster_tag else 'N/A'\n",
    "    last_posters.append(last_poster)\n",
    "\n",
    "# Display extracted information\n",
    "for i in range(len(poster_names)):\n",
    "    print(f\"Poster Name: {poster_names[i]}\")\n",
    "    print(f\"Post Count: {post_counts[i]}\")\n",
    "    print(f\"Views: {view_counts[i]}\")\n",
    "    print(f\"Time: {times[i]}\")\n",
    "    print(f\"Date: {dates[i]}\")\n",
    "    print(f\"Last Poster Name: {last_posters[i]}\")\n",
    "    print('-' * 50)\n",
    "\n",
    "    \n",
    "import requests\n",
    "\n",
    "cookies = {\n",
    "    'session': '\"\"',\n",
    "    'ai_user': 'e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z',\n",
    "    'session': '\"\"',\n",
    "    '_gid': 'GA1.2.927634570.1727849250',\n",
    "    '__gads': 'ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA',\n",
    "    '__gpi': 'UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w',\n",
    "    '__eoi': 'ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44',\n",
    "    'FCNEC': '%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D',\n",
    "    '_gat_gtag_UA_91638_1': '1',\n",
    "    '_ga_GGM2XGB2C1': 'GS1.1.1727856871.4.1.1727858023.0.0.0',\n",
    "    '_ga': 'GA1.1.117970615.1727064250',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    # 'cookie': 'session=\"\"; ai_user=e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z; session=\"\"; _gid=GA1.2.927634570.1727849250; __gads=ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA; __gpi=UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w; __eoi=ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44; FCNEC=%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D; _gat_gtag_UA_91638_1=1; _ga_GGM2XGB2C1=GS1.1.1727856871.4.1.1727858023.0.0.0; _ga=GA1.1.117970615.1727064250',\n",
    "    'priority': 'u=0, i',\n",
    "    'referer': 'https://www.nairaland.com/jobs',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "response = requests.get('https://www.nairaland.com/jobs/1', cookies=cookies, headers=headers)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "print(soup)\n",
    "\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Extract the main poster's name\n",
    "poster_name_tag = soup.find('a', href='/curdlebug')\n",
    "poster_name = poster_name_tag.text if poster_name_tag else 'N/A'\n",
    "\n",
    "# Extract the number of posts and views\n",
    "post_views_info = soup.find('span', class_='s')\n",
    "posts_and_views = post_views_info.find_all('b')\n",
    "post_count = posts_and_views[1].text if len(posts_and_views) > 1 else 'N/A'\n",
    "views = posts_and_views[2].text if len(posts_and_views) > 2 else 'N/A'\n",
    "\n",
    "# Extract the time and date of the post\n",
    "time = post_views_info.find_all('b')[3].text if len(posts_and_views) > 3 else 'N/A'\n",
    "date = post_views_info.find_all('b')[4].text if len(posts_and_views) > 4 else 'N/A'\n",
    "\n",
    "# Extract the second poster's name (last poster)\n",
    "last_poster_tag_name = soup.find('a', href= '/dharmie13')  # Get the last poster from the <a> tag list\n",
    "last_poster = last_poster_tag_name.text if last_poster_tag_name else 'N/A'\n",
    "\n",
    "# Print the extracted data\n",
    "print(f\"Poster Name: {poster_name}\")\n",
    "print(f\"Post Count: {post_count}\")\n",
    "print(f\"Views: {views}\")\n",
    "print(f\"Time: {time}\")\n",
    "#print(f\"Date: {date}\")\n",
    "print(f\"Last Poster Name: {last_poster}\")\n",
    "\n",
    "\n",
    "\n",
    "# Parse the HTML with BeautifulSoup\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Find all post blocks (adjust the tag and class according to actual structure)\n",
    "posts = soup.find_all('span', class_='s')  # Assuming each post details are within <span> with class 's'\n",
    "\n",
    "# Initialize lists to store the extracted information\n",
    "poster_names = []\n",
    "post_counts = []\n",
    "view_counts = []\n",
    "times = []\n",
    "dates = []\n",
    "last_posters = []\n",
    "\n",
    "# Loop through each post block to extract information\n",
    "for post_views_info in posts:\n",
    "    # Extract poster's name\n",
    "    poster_name_tag = post_views_info.find('a')  # Assuming the first <a> tag is the poster's name\n",
    "    poster_name = poster_name_tag.text if poster_name_tag else 'N/A'\n",
    "    poster_names.append(poster_name)\n",
    "\n",
    "    # Extract the number of posts and views\n",
    "    posts_and_views = post_views_info.find_all('b')\n",
    "    post_count = posts_and_views[1].text if len(posts_and_views) > 1 else 'N/A'\n",
    "    post_counts.append(post_count)\n",
    "\n",
    "    view_count = posts_and_views[2].text if len(posts_and_views) > 2 else 'N/A'\n",
    "    view_counts.append(view_count)\n",
    "\n",
    "    # Extract the time and date of the post\n",
    "    time = posts_and_views[3].text if len(posts_and_views) > 3 else 'N/A'\n",
    "    times.append(time)\n",
    "\n",
    "    date = posts_and_views[4].text if len(posts_and_views) > 4 else 'N/A'\n",
    "    dates.append(date)\n",
    "\n",
    "    # Extract the second poster's name (last poster)\n",
    "    last_poster_tag = post_views_info.find_all('a')[-1]  # Assuming the last <a> tag is the last poster's name\n",
    "    last_poster = last_poster_tag.text if last_poster_tag else 'N/A'\n",
    "    last_posters.append(last_poster)\n",
    "\n",
    "# Display extracted information\n",
    "for i in range(len(poster_names)):\n",
    "    print(f\"Poster Name: {poster_names[i]}\")\n",
    "    print(f\"Post Count: {post_counts[i]}\")\n",
    "    print(f\"Views: {view_counts[i]}\")\n",
    "    print(f\"Time: {times[i]}\")\n",
    "    print(f\"Date: {dates[i]}\")\n",
    "    print(f\"Last Poster Name: {last_posters[i]}\")\n",
    "    print('-' * 50)\n",
    "    \n",
    "    \n",
    "# connecting to mySQL Database \n",
    "import mysql.connector\n",
    "\n",
    "mydatabase = mysql.connector.connect(\n",
    "  host=\"localhost\", # database host\n",
    "  user=\"root\", # username\n",
    "  password=\"****\", # password\n",
    "  port = \"3306\",\n",
    ")\n",
    "\n",
    "print(mydatabase)\n",
    "\n",
    "\n",
    "#creating the database w3_schools\n",
    "mycursor = mydatabase.cursor() # SQL database connector\n",
    "\n",
    "mycursor.execute(\"CREATE DATABASE Nairaland_Jobs\")\n",
    "\n",
    "\n",
    "# showing all the databases in MySQL\n",
    "mycursor = mydatabase.cursor()\n",
    "mycursor.execute(\"SHOW DATABASES\")\n",
    "\n",
    "for x in mycursor:\n",
    "  print(x)\n",
    "\n",
    "\n",
    "\n",
    "# Try connecting to the database \"nairaland jobs\":\n",
    "mydatabase = mysql.connector.connect(  \n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"****\",\n",
    "  port = \"3306\",\n",
    "  database=\"nairaland_jobs\"                                 \n",
    ")\n",
    "\n",
    "\n",
    "mycursor = mydatabase.cursor()\n",
    "\n",
    "# Corrected CREATE TABLE statement\n",
    "mycursor.execute(\"\"\"\n",
    "    CREATE TABLE Jobs (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY, \n",
    "    poster_name VARCHAR(255),\n",
    "    post_count INT,\n",
    "    view_count VARCHAR(225),\n",
    "    time_posted VARCHAR(50),\n",
    "    date_posted VARCHAR(50),\n",
    "    last_poster_name VARCHAR(255)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "mycursor = mydatabase.cursor()\n",
    "mycursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "for x in mycursor:\n",
    "  print(x)\n",
    "\n",
    "\n",
    "# PYTHON SCRIPTS PIPELINE FOR THE FIRST TABLE (1) JOBS\n",
    "\n",
    "import mysql.connector\n",
    "import requests\n",
    "\n",
    "\n",
    "cookies = {\n",
    "    'ai_user': 'e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z',\n",
    "    'session': '\"\"',\n",
    "    '_gid': 'GA1.2.927634570.1727849250',\n",
    "    '__gads': 'ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA',\n",
    "    '__gpi': 'UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w',\n",
    "    '__eoi': 'ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44',\n",
    "    'FCNEC': '%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D',\n",
    "    '_ga_GGM2XGB2C1': 'GS1.1.1727849249.3.1.1727849305.0.0.0',\n",
    "    '_ga': 'GA1.1.117970615.1727064250',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    # 'cookie': 'ai_user=e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z; session=\"\"; _gid=GA1.2.927634570.1727849250; __gads=ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA; __gpi=UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w; __eoi=ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44; FCNEC=%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D; _ga_GGM2XGB2C1=GS1.1.1727849249.3.1.1727849305.0.0.0; _ga=GA1.1.117970615.1727064250',\n",
    "    'priority': 'u=0, i',\n",
    "    'referer': 'https://www.nairaland.com/',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "# MySQL connection setup\n",
    "def connect_to_db():\n",
    "    mydatabase = mysql.connector.connect(  \n",
    "      host=\"localhost\",\n",
    "      user=\"root\",\n",
    "      password=\"****\",\n",
    "      port = \"3306\",\n",
    "      database=\"nairaland_jobs\"                                 \n",
    "    )\n",
    "    return  mydatabase\n",
    "\n",
    "# Function to insert data into the database\n",
    "def insert_data_into_db(cursor, poster_name, post_count, view_count, time, date, last_poster):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO jobs (poster_name, post_count, view_count, time_posted, date_posted, last_poster_name)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (poster_name, post_count, view_count, time, date, last_poster))\n",
    "\n",
    "# Function to extract data from a single page\n",
    "def extract_data_from_html(response, cursor):\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    posts = soup.find_all('span', class_='s')  # Adjust the tag and class as per actual HTML structure\n",
    "    \n",
    "    # Loop through each post block to extract information\n",
    "    for post_views_info in posts:\n",
    "        # Extract poster's name\n",
    "        poster_name_tag = post_views_info.find('a')  # Assuming the first <a> tag is the poster's name\n",
    "        poster_name = poster_name_tag.text if poster_name_tag else 'N/A'\n",
    "\n",
    "        # Extract the number of posts and views\n",
    "        posts_and_views = post_views_info.find_all('b')\n",
    "        post_count = posts_and_views[1].text if len(posts_and_views) > 1 else 'N/A'\n",
    "        view_count = posts_and_views[2].text if len(posts_and_views) > 2 else 'N/A'\n",
    "\n",
    "        # Extract the time and date of the post\n",
    "        time = posts_and_views[3].text if len(posts_and_views) > 3 else 'N/A'\n",
    "        date = posts_and_views[4].text if len(posts_and_views) > 4 else 'N/A'\n",
    "\n",
    "        # Extract the second poster's name (last poster)\n",
    "        last_poster_tag = post_views_info.find_all('a')[-1]  # Assuming the last <a> tag is the last poster's name\n",
    "        last_poster = last_poster_tag.text if last_poster_tag else 'N/A'\n",
    "\n",
    "        # Insert the data into the database\n",
    "        insert_data_into_db(cursor, poster_name, post_count, view_count, time, date, last_poster)\n",
    "\n",
    "# Main function to scrape a single page and insert data into the database\n",
    "def scrape_single_page(url, cookies, headers):\n",
    "    connection = connect_to_db()\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    try:\n",
    "        # Fetch the page using cookies and headers\n",
    "        response = requests.get(url, cookies=cookies, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract data from the HTML content and insert into DB\n",
    "        extract_data_from_html(response, cursor)  # Pass the cursor to extract_data_from_html()\n",
    "\n",
    "        connection.commit()  # Commit the changes after inserting data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to retrieve data from {url}: {e}\")\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.nairaland.com/jobs'\n",
    "\n",
    "# Start scraping the single page and storing it in the database\n",
    "scrape_single_page(url, cookies, headers)\n",
    "\n",
    "\n",
    "mycursor = mydatabase.cursor()\n",
    "\n",
    "# Corrected CREATE TABLE statement\n",
    "mycursor.execute(\"\"\"\n",
    "    CREATE TABLE Jobs_2 (\n",
    "    id INT AUTO_INCREMENT PRIMARY KEY, \n",
    "    poster_name VARCHAR(255),\n",
    "    post_count INT,\n",
    "    view_count VARCHAR(225),\n",
    "    time_posted VARCHAR(50),\n",
    "    date_posted VARCHAR(50),\n",
    "    last_poster_name VARCHAR(255)\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "mycursor = mydatabase.cursor()\n",
    "mycursor.execute(\"SHOW TABLES\")\n",
    "\n",
    "for x in mycursor:\n",
    "  print(x)\n",
    "\n",
    "\n",
    "# PYTHON SCRIPTS PIPELINE FOR THE SECOND TABLE (2) JOBS_2\n",
    "\n",
    "import mysql.connector\n",
    "import requests\n",
    "\n",
    "cookies = {\n",
    "    'session': '\"\"',\n",
    "    'ai_user': 'e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z',\n",
    "    'session': '\"\"',\n",
    "    '_gid': 'GA1.2.927634570.1727849250',\n",
    "    '__gads': 'ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA',\n",
    "    '__gpi': 'UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w',\n",
    "    '__eoi': 'ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44',\n",
    "    'FCNEC': '%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D',\n",
    "    '_gat_gtag_UA_91638_1': '1',\n",
    "    '_ga_GGM2XGB2C1': 'GS1.1.1727856871.4.1.1727858023.0.0.0',\n",
    "    '_ga': 'GA1.1.117970615.1727064250',\n",
    "}\n",
    "\n",
    "headers = {\n",
    "    'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "    'accept-language': 'en-US,en;q=0.9',\n",
    "    'cache-control': 'max-age=0',\n",
    "    # 'cookie': 'session=\"\"; ai_user=e2Hi55QF1yZg+yBbxztwp6|2024-09-23T04:04:09.869Z; session=\"\"; _gid=GA1.2.927634570.1727849250; __gads=ID=b82f70ee813806ec:T=1727064691:RT=1727849696:S=ALNI_MbX4hXYSDlvWE9FqwH08BS8-VgnMA; __gpi=UID=00000f096ea6a392:T=1727064691:RT=1727849696:S=ALNI_MYDy964LQKQu4a5evvNhU6RiNAV6w; __eoi=ID=9c42286dda43baf5:T=1727064691:RT=1727849696:S=AA-AfjYpPn8xZaYyKA3TAmN8o-44; FCNEC=%5B%5B%22AKsRol9TcnUvFlUTk2YxxAWVCmcIdyFo_sk4-wyu9PdZa4HUZtqNoi-f4sc1glHbztVAD8XFdexFjlGlK39rbbVbSLzLutPd8qgsNjWLkGFi_iULFKPA3j5OfMMBi-KxR-Lz51PBU415C5_AvZ84EaKE5aGuz_OodQ%3D%3D%22%5D%5D; _gat_gtag_UA_91638_1=1; _ga_GGM2XGB2C1=GS1.1.1727856871.4.1.1727858023.0.0.0; _ga=GA1.1.117970615.1727064250',\n",
    "    'priority': 'u=0, i',\n",
    "    'referer': 'https://www.nairaland.com/jobs',\n",
    "    'sec-ch-ua': '\"Google Chrome\";v=\"129\", \"Not=A?Brand\";v=\"8\", \"Chromium\";v=\"129\"',\n",
    "    'sec-ch-ua-mobile': '?0',\n",
    "    'sec-ch-ua-platform': '\"Windows\"',\n",
    "    'sec-fetch-dest': 'document',\n",
    "    'sec-fetch-mode': 'navigate',\n",
    "    'sec-fetch-site': 'same-origin',\n",
    "    'sec-fetch-user': '?1',\n",
    "    'upgrade-insecure-requests': '1',\n",
    "    'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/129.0.0.0 Safari/537.36',\n",
    "}\n",
    "\n",
    "# MySQL connection setup\n",
    "def connect_to_db():\n",
    "    mydatabase = mysql.connector.connect(  \n",
    "      host=\"localhost\",\n",
    "      user=\"root\",\n",
    "      password=\"****\",\n",
    "      port = \"3306\",\n",
    "      database=\"nairaland_jobs\"                                  \n",
    "    )\n",
    "    return  mydatabase\n",
    "\n",
    "# Function to insert data into the database\n",
    "def insert_data_into_db(cursor, poster_name, post_count, view_count, time, date, last_poster):\n",
    "    insert_query = \"\"\"\n",
    "    INSERT INTO jobs_2 (poster_name, post_count, view_count, time_posted, date_posted, last_poster_name)\n",
    "    VALUES (%s, %s, %s, %s, %s, %s)\n",
    "    \"\"\"\n",
    "    cursor.execute(insert_query, (poster_name, post_count, view_count, time, date, last_poster))\n",
    "\n",
    "# Function to extract data from a single page\n",
    "def extract_data_from_html(response, cursor):\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    posts = soup.find_all('span', class_='s')  # Adjust the tag and class as per actual HTML structure\n",
    "    \n",
    "    # Loop through each post block to extract information\n",
    "    for post_views_info in posts:\n",
    "        # Extract poster's name\n",
    "        poster_name_tag = post_views_info.find('a')  # Assuming the first <a> tag is the poster's name\n",
    "        poster_name = poster_name_tag.text if poster_name_tag else 'N/A'\n",
    "\n",
    "        # Extract the number of posts and views\n",
    "        posts_and_views = post_views_info.find_all('b')\n",
    "        post_count = posts_and_views[1].text if len(posts_and_views) > 1 else 'N/A'\n",
    "        view_count = posts_and_views[2].text if len(posts_and_views) > 2 else 'N/A'\n",
    "\n",
    "        # Extract the time and date of the post\n",
    "        time = posts_and_views[3].text if len(posts_and_views) > 3 else 'N/A'\n",
    "        date = posts_and_views[4].text if len(posts_and_views) > 4 else 'N/A'\n",
    "\n",
    "        # Extract the second poster's name (last poster)\n",
    "        last_poster_tag = post_views_info.find_all('a')[-1]  # Assuming the last <a> tag is the last poster's name\n",
    "        last_poster = last_poster_tag.text if last_poster_tag else 'N/A'\n",
    "\n",
    "        # Insert the data into the database\n",
    "        insert_data_into_db(cursor, poster_name, post_count, view_count, time, date, last_poster)\n",
    "\n",
    "# Main function to scrape a single page and insert data into the database\n",
    "def scrape_single_page(url, cookies, headers):\n",
    "    connection = connect_to_db()\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    try:\n",
    "        # Fetch the page using cookies and headers\n",
    "        response = requests.get(url, cookies=cookies, headers=headers)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Extract data from the HTML content and insert into DB\n",
    "        extract_data_from_html(response, cursor)  # Pass the cursor to extract_data_from_html()\n",
    "\n",
    "        connection.commit()  # Commit the changes after inserting data\n",
    "\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Failed to retrieve data from {url}: {e}\")\n",
    "    \n",
    "    cursor.close()\n",
    "    connection.close()\n",
    "\n",
    "# URL of the page to scrape\n",
    "url = 'https://www.nairaland.com/jobs/1'\n",
    "\n",
    "# Start scraping the single page and storing it in the database\n",
    "scrape_single_page(url, cookies, headers)\n",
    "\n",
    "\n",
    "# Use the DESC keyword to sort the result in a descending order.\n",
    "import mysql.connector\n",
    "\n",
    "mydatabase = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"****\",\n",
    "  port = \"3306\",\n",
    "  database=\"nairaland_jobs\" \n",
    "\n",
    ")\n",
    "\n",
    "mycursor = mydatabase.cursor()\n",
    "\n",
    "sql = \"SELECT * FROM jobs ORDER BY view_count  DESC\"\n",
    "\n",
    "mycursor.execute(sql)\n",
    "\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "for x in myresult:\n",
    "  print(x)\n",
    "\n",
    "\n",
    "# Use the DESC keyword to sort the result in a descending order.\n",
    "import mysql.connector\n",
    "\n",
    "mydatabase = mysql.connector.connect(\n",
    "  host=\"localhost\",\n",
    "  user=\"root\",\n",
    "  password=\"****\",\n",
    "  port = \"3306\",\n",
    "  database=\"nairaland_jobs\" \n",
    "\n",
    ")\n",
    "\n",
    "mycursor = mydatabase.cursor()\n",
    "\n",
    "sql = \"SELECT * FROM jobs_2 ORDER BY view_count  DESC\"\n",
    "\n",
    "mycursor.execute(sql)\n",
    "\n",
    "myresult = mycursor.fetchall()\n",
    "\n",
    "for x in myresult:\n",
    "  print(x)\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
